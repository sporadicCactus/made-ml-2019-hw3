{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train.pickle', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_size_dict = dict()\n",
    "for qid, docs in dataset.items():\n",
    "    labels = np.array([doc[0] for doc in docs])\n",
    "    features = np.vstack([doc[1] for doc in docs])\n",
    "    sort_ind = np.flip(np.argsort(labels))\n",
    "    labels = labels[sort_ind].reshape(-1,1)\n",
    "    if labels.max() == labels.min():\n",
    "        continue\n",
    "    features = features[sort_ind]\n",
    "    if labels.shape[0] in data_by_size_dict.keys():\n",
    "        data_by_size_dict[labels.shape[0]][0].append(features)\n",
    "        data_by_size_dict[labels.shape[0]][1].append(labels)\n",
    "    else:\n",
    "        data_by_size_dict[labels.shape[0]] = ([features], [labels])\n",
    "keys = list(data_by_size_dict.keys())\n",
    "keys.sort()\n",
    "data_by_size = []\n",
    "for key in keys:\n",
    "    value = data_by_size_dict[key]\n",
    "    data_by_size.append((np.array(value[0]), np.array(value[1])))\n",
    "del data_by_size_dict\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = data_by_size[0][0].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt_split(data_list, test_size):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for data in data_list:\n",
    "        if len(data[0]) < 10:\n",
    "            test_list.append((torch.FloatTensor(data[0]).cuda(),\n",
    "                              torch.LongTensor(data[1]).cuda()))\n",
    "            continue\n",
    "        X_train, X_test = train_test_split(data[0], test_size = test_size, train_size=1 - test_size)\n",
    "        y_train, y_test = train_test_split(data[1], test_size = test_size, train_size=1 - test_size)\n",
    "        train_list.append((torch.FloatTensor(X_train).cuda(),torch.LongTensor(y_train).cuda()))\n",
    "        test_list.append((torch.FloatTensor(X_test).cuda(),torch.LongTensor(y_test).cuda()))\n",
    "    return train_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, test_list = tt_split(data_by_size, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935.1435546875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()/2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nDCG(model, data_list):\n",
    "    model = model.eval()\n",
    "    log2 = torch.tensor(np.log(2)).cuda()\n",
    "    acc_list = []\n",
    "    for batch in data_list:   \n",
    "        with torch.no_grad():\n",
    "            features = batch[0]\n",
    "            labels = batch[1]\n",
    "            scores = model.forward(features)\n",
    "            n_docs = labels.shape[1]\n",
    "            order = 1 + torch.sort(torch.sort(scores, dim=1, descending=True)[1], dim=1)[1]\n",
    "            true_order = torch.arange(1, labels.shape[1] + 1).reshape(-1,1)\\\n",
    "                .repeat(labels.shape[0],1,1).cuda()\n",
    "            explabel = torch.exp(labels*log2)\n",
    "            logorder = 1 / (torch.log(order.float()) + 1)\n",
    "            DCGmax = ((explabel - 1)/(torch.log(true_order.float()) + 1))\\\n",
    "                .sum(dim=1, keepdim=True)\n",
    "            DCG = ((explabel - 1)/(torch.log(order.float()) + 1)).sum(dim=1, keepdim=True)\n",
    "            acc_list.append((DCG.shape[0], ((DCG+0.001)/(DCGmax+0.001)).sum().item()))\n",
    "    acc = 0\n",
    "    total_queries = 0\n",
    "    for n_queries, nDCGsum in acc_list:\n",
    "        total_queries += n_queries\n",
    "        acc += nDCGsum\n",
    "    return acc/total_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_features, n_layers, layer_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        layer_struct = [layer_size for i in range(n_layers)]\n",
    "        layer_struct = [in_features] + layer_struct\n",
    "        for i in range(1, n_layers + 1):\n",
    "            self.layers.append(nn.Linear(layer_struct[i-1], layer_struct[i]))\n",
    "        self.out_layer = nn.Linear(layer_struct[-1], 1)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = x\n",
    "        for layer in self.layers:\n",
    "            y_pred = layer(y_pred)\n",
    "            y_pred = y_pred + torch.relu(y_pred)\n",
    "            y_pred = self.drop(y_pred)\n",
    "        y_pred = self.out_layer(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_features, 10, 500).cuda()\n",
    "curr_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14857\n",
      "12086\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(batch[0]) for batch in train_list]))\n",
    "print(sum([len(batch[0]) for batch in train_list[3:40]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.82398, test: 0.81869\n",
      "Train: 0.82394, test: 0.81889\n",
      "Train: 0.82398, test: 0.81893\n",
      "Train: 0.82410, test: 0.81901\n",
      "Train: 0.82408, test: 0.81887\n",
      "Train: 0.82399, test: 0.81895\n",
      "Train: 0.82400, test: 0.81887\n",
      "Train: 0.82395, test: 0.81896\n",
      "Train: 0.82386, test: 0.81889\n",
      "Train: 0.82396, test: 0.81889\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000008)\n",
    "\n",
    "log2 = torch.tensor(np.log(2)).cuda()\n",
    "epochs = 10\n",
    "\n",
    "optimizer.zero_grad()\n",
    "for ep in range(epochs):\n",
    "    for batch in train_list[3:40]:\n",
    "        model.train()\n",
    "        features = batch[0]\n",
    "        labels = batch[1]\n",
    "        scores = model.forward(features)\n",
    "        n_docs = labels.shape[1]\n",
    "        with torch.no_grad():\n",
    "            order = 1 + torch.sort(torch.sort(scores, dim=1, descending=True)[1], dim=1)[1]\n",
    "            true_order = torch.arange(1, labels.shape[1] + 1).reshape(-1,1)\\\n",
    "                .repeat(labels.shape[0],1,1).cuda()\n",
    "            explabel = torch.exp(labels*log2)\n",
    "            logorder = 1 / (torch.log(order.float()) + 1)\n",
    "            DCGmax = ((explabel - 1)/(torch.log(true_order.float()) + 1)).sum(dim=1, keepdim=True)\n",
    "            dNDCGabs = ((logorder.repeat(1,1,n_docs) - logorder.repeat(1,1,n_docs).transpose(1,2)) * \\\n",
    "                (explabel.repeat(1,1,n_docs) - explabel.repeat(1,1,n_docs).transpose(1,2))).abs()/(0.01 + DCGmax)\n",
    "            dscores = scores.repeat(1,1,n_docs) - scores.repeat(1,1,n_docs).transpose(1,2)\n",
    "            smat = torch.sign(labels.repeat(1,1,n_docs) - labels.repeat(1,1,n_docs).transpose(1,2))\n",
    "            lambdaij = smat*torch.sigmoid(-smat*dscores)*dNDCGabs\n",
    "            lambdai = lambdaij.sum(dim=1) - lambdaij.sum(dim=2)\n",
    "        loss = (scores.view(-1,n_docs)*lambdai).sum()\n",
    "        loss = loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    test_metric = nDCG(model, test_list)\n",
    "    print(f'Train: {nDCG(model, train_list):.5f}, test: {test_metric:.5f}')\n",
    "    if test_metric > curr_test:\n",
    "        curr_test = test_metric\n",
    "        torch.save(model.state_dict(), 'temp.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8188916741338881"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nDCG(model, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('temp.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test.pickle', 'rb') as file:\n",
    "    testset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2 = np.log(2)\n",
    "docs_processed = 0\n",
    "model.eval()\n",
    "filename = 'netsol_3.txt'\n",
    "with open('./'+filename, 'w') as file:\n",
    "    file.write('QueryId,DocumentId\\n')\n",
    "    for qid, docs in testset.items():\n",
    "        X = torch.tensor(np.vstack(tuple((doc[1] for doc in docs)))).cuda().float()\n",
    "        doc_ids = []\n",
    "        for doc in docs:\n",
    "            docs_processed += 1\n",
    "            doc_ids.append(docs_processed)\n",
    "        doc_ids = np.array(doc_ids).reshape(-1)\n",
    "        scores = model(X).reshape(-1)\n",
    "#         scores = torch.rand(len(doc_ids))\n",
    "        ordered_index = torch.sort(scores, descending=True)[1]\n",
    "        ordered_ids = doc_ids[ordered_index.cpu()].reshape(-1)\n",
    "        for i in range(len(ordered_ids)):\n",
    "            file.write(str(qid)+','+str(ordered_ids[i])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
